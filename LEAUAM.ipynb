{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "attention.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBm9oaQfe486",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "18b78cf9-998a-48d4-e3d6-375af9daa146"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input,Dense,LSTM,GRU,Embedding,Bidirectional,RepeatVector,Concatenate,Activation,Dot,Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as k\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5oSBlyvfynK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "67b5538f-0637-4452-e590-796c32248566"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/data.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr9DHrHrf7pK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('/content/data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D93qEScagAyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns=['utterance','emotion']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV7XYIV2gA5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['emotion']=data.emotion.replace({1:'angry',2:'sad',3:'happy',4:'worry',0:'neutral'})\n",
        "data_worry=data[data.emotion=='worry']\n",
        "data_angry=data[data.emotion=='angry'][:1000]\n",
        "data_sad=data[data.emotion=='sad'][:1000]\n",
        "data_happy=data[data.emotion=='happy'][:1000]\n",
        "data_neutral=data[data.emotion=='neutral'][:1000\n",
        "                                           ]\n",
        "final_data=pd.concat([data_worry,data_neutral,data_sad,data_happy,data_angry])\n",
        "from sklearn.utils import shuffle\n",
        "final_data=shuffle(final_data)\n",
        "final_data.to_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YD124kKJEiN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "995bef19-6566-46a5-8418-69d53e1334dd"
      },
      "source": [
        "final_data.emotion.value_counts()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "worry      1108\n",
              "neutral    1000\n",
              "sad        1000\n",
              "happy      1000\n",
              "angry      1000\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWYxNEGrgAvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=final_data.utterance.astype(str)\n",
        "y=final_data.emotion.astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-HH4Ja_got-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "out=[]\n",
        "labels=[]\n",
        "\n",
        "for w in y:\n",
        "    \n",
        "    out.append(w)\n",
        "for l in out:\n",
        "    target_text='<START> '+ l +' <END>'\n",
        "   \n",
        "    labels.append(target_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJxr_fFVgrwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts=[]\n",
        "for line in x:\n",
        "    texts.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-QCcsiShODN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer=Tokenizer(num_words=40000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaPkGLqRhQwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(texts+labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9StDpdlBhTsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2558f0e9-e1de-48cd-9aa7-984e35a650fa"
      },
      "source": [
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB SIZE : 9602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7sS_PVhTvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a14d841c-7c08-48a5-8874-dc80678421f0"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-01 04:47:45--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-01 04:47:45--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-01 04:47:45--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.01MB/s    in 6m 28s  \n",
            "\n",
            "2020-02-01 04:54:13 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP4AxcxlhT0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "74a09d66-6e18-4cb8-b53f-5d6790b23ecd"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIK8IX2DhT4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6997f377-c9b3-4fe4-fd73-1fedad89b4af"
      },
      "source": [
        "print('Indexing word vectors.')\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.200d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1hMjXUNjGru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words_input=len(tokenizer.word_index)+1\n",
        "word2idx_input=tokenizer.word_index\n",
        "MAX_NUM_WORDS=40000\n",
        "num_words=num_words_input\n",
        "embedding_matrix=np.zeros((num_words,200))\n",
        "for word,i in word2idx_input.items():\n",
        "    if i<MAX_NUM_WORDS:\n",
        "        embedding_vector=embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i]=embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiKU2pTCjKsD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99a5d6bf-c3bd-411e-a33d-3ee4c5fcf1d9"
      },
      "source": [
        "from keras import preprocessing,utils\n",
        "import numpy as np\n",
        "tokenized_questions = tokenizer.texts_to_sequences( texts )\n",
        "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
        "encoder_input_data = np.array( padded_questions )\n",
        "print( encoder_input_data.shape , maxlen_questions )\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( labels)\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )\n",
        "\n",
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( labels)\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_target_one_hot=np.zeros((len(texts),maxlen_answers,VOCAB_SIZE),dtype='float32')\n",
        "for i,d in enumerate(padded_answers):\n",
        "    for t,word in enumerate(d):\n",
        "        decoder_target_one_hot[i,t,word]=1\n",
        "\n",
        "# Saving all the arrays to storage\n",
        "np.save( 'enc_in_data.npy' , encoder_input_data )\n",
        "np.save( 'dec_in_data.npy' , decoder_input_data )\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5108, 36) 36\n",
            "(5108, 3) 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_rou19-e49E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_P4tg7He49J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make sure that we do softmax over time axis = 1\n",
        "#expected shape N X T X D\n",
        "# N=no. of samples\n",
        "#T=sequence length\n",
        "#D vector dimensionality\n",
        "def softmax_over_time(x):\n",
        "    assert(k.ndim(x)>2)\n",
        "    \n",
        "    e=k.exp(x-k.max(x,axis=1,keepdims=True))\n",
        "    s=k.sum(e,axis=1,keepdims=True)\n",
        "    return e/s  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FEs4V6Je49O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=64\n",
        "EPOCHS=1\n",
        "LATENT_DIM=256\n",
        "LATENT_DIM_DECODER=256\n",
        "NUM_SAMPLES=10000\n",
        "MAX_SEQUENCE_LENGTH=100\n",
        "MAX_NUM_WORDS=20000\n",
        "EMBEDDING_DIM=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR2gzfsie4_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating embedding layer\n",
        "embedding_layer=Embedding(VOCAB_SIZE,256,weights=[embedding_matrix],input_length=maxlen_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VALlsVUVe4_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " ###building model\n",
        "#setup encoder simple\n",
        "encoder_input_placeholder=Input(shape=(maxlen_questions,))\n",
        "x=embedding_layer(encoder_input_placeholder)\n",
        "encoder=Bidirectional(LSTM(LATENT_DIM,return_sequences=True,dropout=0.3))\n",
        "encoder_outputs=encoder(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KefF24pee4_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_inputs_placeholder=Input(shape=(maxlen_answers,))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFuk-upue4_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_embedding=Embedding(VOCAB_SIZE,256)\n",
        "decoder_inputs_x=decoder_embedding(decoder_inputs_placeholder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O_yf6NRe4_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#attention layer\n",
        "attn_repeat_layer=RepeatVector(maxlen_questions)\n",
        "attn_concat_layer=Concatenate(axis=-1)\n",
        "attn_dense1=Dense(10,activation='tanh')\n",
        "attn_dense2=Dense(1,activation=softmax_over_time)\n",
        "attn_dot=Dot(axes=1)#to perform the weighted sum of alpha(t)*h(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ciy3vPcIe4_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(h,st_1):\n",
        "    #h=h(1)......h(Tx),shape=(Tx,LATEND_DIM*2)\n",
        "    #ST_1=s(t-1),shape=(LATENT_DIM_DECODER,)\n",
        "    \n",
        "    #copy s(t-1) tx times\n",
        "    #now shape=(Tx,LATENT_DIM_DECODER)\n",
        "    st_1=attn_repeat_layer(st_1)\n",
        "    \n",
        "    #concat all h(t)'s with s(t-1)\n",
        "    #now shape (tx,LATENT_DIM_DECODER+LATENT_DIM*2)\n",
        "    x=attn_concat_layer([h,st_1])\n",
        "    \n",
        "    #neural net first layer\n",
        "    x=attn_dense1(x)\n",
        "    \n",
        "    #neural net second layer with special softmax over time\n",
        "    alphas=attn_dense2(x)\n",
        "    \n",
        "    #Dot the alphas and h's\n",
        "    #a.dot(b)=sum over a[t]*b[t]\n",
        "    \n",
        "    context = attn_dot([alphas,h])\n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbe-d2xLe4_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define rest of the decoder(after attention)\n",
        "from keras import regularizers\n",
        "decoder_lstm=LSTM(256,return_state=True,dropout=0.25)\n",
        "decoder_dense=Dense(VOCAB_SIZE,activation='softmax',activity_regularizer=regularizers.l2(0.00001),kernel_regularizer=regularizers.l2(0.00001))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWnfxIrie4_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_s=Input(shape=(256,),name='s0')\n",
        "initial_c=Input(shape=(256,),name='c0')\n",
        "context_last_word_concat_layer=Concatenate(axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp30ypxqe4__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# s and c will reassign after each step\n",
        "s=initial_s\n",
        "c=initial_c\n",
        "outputs=[]\n",
        "\n",
        "#collect output in a list at first\n",
        "for t in range(maxlen_answers): #ty times\n",
        "    #get the context using attention mech\n",
        "    context=one_step_attention(encoder_outputs,s)\n",
        "    \n",
        "    #we need a different layer for each time step\n",
        "    selector=Lambda(lambda x: x[:,t:t+1])\n",
        "    xt=selector(decoder_inputs_x)\n",
        "    \n",
        "    #combine\n",
        "    decoder_lstm_input=context_last_word_concat_layer([context,xt])\n",
        "    \n",
        "    #pass the combined [context,last word] into lstm\n",
        "    #along with [s,c]\n",
        "    #get the new[s,c] and output\n",
        "    o,s,c=decoder_lstm(decoder_lstm_input,initial_state=[s,c])\n",
        "    \n",
        "    #final dense layer to get next word prediction\n",
        "    decoder_outputs=decoder_dense(o)\n",
        "    outputs.append(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LIqBPIxe5AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stack_and_transpose(x):\n",
        "    # x is a lisy of length T, each element is a batch_size x output_vocab_size tensor\n",
        "    x=k.stack(x)\n",
        "    x=k.permute_dimensions(x,pattern=(1,0,2)) # is now batch_size x T x output_vocab_size\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzlpYartkraD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stacker=Lambda(stack_and_transpose)\n",
        "outputs=stacker(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyUiaI6ikte_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Model(inputs=[encoder_input_placeholder,decoder_inputs_placeholder,initial_s,initial_c],outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCdnQULMkyLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klS_57x_k0gH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "2041adf4-f454-4fb9-bd59-84b98539f742"
      },
      "source": [
        "#train the model\n",
        "z=np.zeros((5108,256)) #initial s c\n",
        "r=model.fit([encoder_input_data,decoder_input_data,z,z],decoder_target_one_hot,batch_size=100,epochs=20,validation_split=0.2)\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4086 samples, validate on 1022 samples\n",
            "Epoch 1/20\n",
            "4086/4086 [==============================] - 18s 4ms/step - loss: 2.2476 - acc: 0.5335 - val_loss: 1.0829 - val_acc: 0.6667\n",
            "Epoch 2/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.8535 - acc: 0.7184 - val_loss: 0.6544 - val_acc: 0.7316\n",
            "Epoch 3/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.6017 - acc: 0.7340 - val_loss: 0.5773 - val_acc: 0.7322\n",
            "Epoch 4/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.5315 - acc: 0.7659 - val_loss: 0.4968 - val_acc: 0.7831\n",
            "Epoch 5/20\n",
            "4086/4086 [==============================] - 8s 2ms/step - loss: 0.4588 - acc: 0.7866 - val_loss: 0.4536 - val_acc: 0.7880\n",
            "Epoch 6/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.4244 - acc: 0.8005 - val_loss: 0.4611 - val_acc: 0.7913\n",
            "Epoch 7/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.4117 - acc: 0.8049 - val_loss: 0.4452 - val_acc: 0.7883\n",
            "Epoch 8/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.3884 - acc: 0.8228 - val_loss: 0.4104 - val_acc: 0.8232\n",
            "Epoch 9/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.3337 - acc: 0.8501 - val_loss: 0.3901 - val_acc: 0.8382\n",
            "Epoch 10/20\n",
            "4086/4086 [==============================] - 8s 2ms/step - loss: 0.2872 - acc: 0.8593 - val_loss: 0.3512 - val_acc: 0.8480\n",
            "Epoch 11/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.2671 - acc: 0.8770 - val_loss: 0.3583 - val_acc: 0.8480\n",
            "Epoch 12/20\n",
            "4086/4086 [==============================] - 8s 2ms/step - loss: 0.2394 - acc: 0.8928 - val_loss: 0.3190 - val_acc: 0.8718\n",
            "Epoch 13/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.2097 - acc: 0.9122 - val_loss: 0.3188 - val_acc: 0.9002\n",
            "Epoch 14/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.1789 - acc: 0.9405 - val_loss: 0.2764 - val_acc: 0.9227\n",
            "Epoch 15/20\n",
            "4086/4086 [==============================] - 8s 2ms/step - loss: 0.1345 - acc: 0.9621 - val_loss: 0.2824 - val_acc: 0.9269\n",
            "Epoch 16/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.1102 - acc: 0.9712 - val_loss: 0.2802 - val_acc: 0.9286\n",
            "Epoch 17/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.0934 - acc: 0.9768 - val_loss: 0.3244 - val_acc: 0.9289\n",
            "Epoch 18/20\n",
            "4086/4086 [==============================] - 8s 2ms/step - loss: 0.0751 - acc: 0.9840 - val_loss: 0.2782 - val_acc: 0.9338\n",
            "Epoch 19/20\n",
            "4086/4086 [==============================] - 7s 2ms/step - loss: 0.0656 - acc: 0.9854 - val_loss: 0.2647 - val_acc: 0.9351\n",
            "Epoch 20/20\n",
            "4086/4086 [==============================] - 8s 2ms/step - loss: 0.0469 - acc: 0.9903 - val_loss: 0.2783 - val_acc: 0.9335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAguNU7-RMJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_input_placeholder, encoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnlEmBc6RTzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_outputs_as_input = Input(shape=(maxlen_questions, LATENT_DIM * 2,))\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-bAANERbPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context = one_step_attention(encoder_outputs_as_input, initial_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Ljrrc1RgzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfXk0mY2RmJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
        "decoder_outputs = decoder_dense(o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHkyGvERixM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_model = Model(\n",
        "  inputs=[\n",
        "    decoder_inputs_single,\n",
        "    encoder_outputs_as_input,\n",
        "    initial_s, \n",
        "    initial_c\n",
        "  ],\n",
        "  outputs=[decoder_outputs, s, c]\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I64qNlDlPpEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2words={v:k for k,v in word2idx_input.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW1VNuCvPspc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # Encode the input as state vectors.\n",
        "  enc_out = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  \n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  # NOTE: tokenizer lower-cases all words\n",
        "  target_seq[0, 0] = word2idx_input['start']\n",
        "\n",
        "  # if we get this we break\n",
        "  eos = word2idx_input['end']\n",
        "\n",
        "\n",
        "  # [s, c] will be updated in each loop iteration\n",
        "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
        "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
        "\n",
        "\n",
        "  # Create the translation\n",
        "  output_sentence = []\n",
        "  for _ in range(maxlen_answers):\n",
        "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
        "        \n",
        "\n",
        "    # Get next word\n",
        "    idx = np.argmax(o.flatten())\n",
        "\n",
        "    # End sentence of EOS\n",
        "    if eos == idx:\n",
        "      break\n",
        "\n",
        "    word = ''\n",
        "    if idx > 0:\n",
        "      word = idx2words[idx]\n",
        "      output_sentence.append(word)\n",
        "\n",
        "    # Update the decoder input\n",
        "    # which is just the word just generated\n",
        "    target_seq[0, 0] = idx\n",
        "\n",
        "  return ' '.join(output_sentence)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc_phf7nQknp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a202b80e-4b22-4ea8-aad6-8f6f15abc805"
      },
      "source": [
        "while True:\n",
        "  # Do some test translations\n",
        "  i = np.random.choice(len(texts))\n",
        "  input_seq = padded_questions[i:i+1]\n",
        "  translation = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input sentence:', texts[i])\n",
        "  print('Predicted translation:', translation)\n",
        "  print('Actual translation:', labels[i])\n",
        "\n",
        "  ans = input(\"Continue? [Y/n]\")\n",
        "  if ans and ans.lower().startswith('n'):\n",
        "    break"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: have you heard kook crazy things original mix by on sound cloud np\n",
            "Predicted translation: angry\n",
            "Actual translation: <START> angry <END>\n",
            "Continue? [Y/n]y\n",
            "-\n",
            "Input sentence: i m horrible at studying hate studying give me 3 h o u r s im good\n",
            "Predicted translation: happy\n",
            "Actual translation: <START> happy <END>\n",
            "Continue? [Y/n]y\n",
            "-\n",
            "Input sentence: does cbd reduce anxiousness anxiety does cbd reduce anxiousness brought on by consuming high amounts of\n",
            "Predicted translation: sad\n",
            "Actual translation: <START> sad <END>\n",
            "Continue? [Y/n]y\n",
            "-\n",
            "Input sentence: teens anxiety how to help your kids anxiety world renowned psychologist has stunning\n",
            "Predicted translation: sad\n",
            "Actual translation: <START> sad <END>\n",
            "Continue? [Y/n]y\n",
            "-\n",
            "Input sentence: did you survive the great quake of 2018 worried\n",
            "Predicted translation: sad\n",
            "Actual translation: <START> sad <END>\n",
            "Continue? [Y/n]y\n",
            "-\n",
            "Input sentence: hey kindly help me spread my new single to your amazing fans i believe they will love it happy thankyou ht\n",
            "Predicted translation: angry\n",
            "Actual translation: <START> angry <END>\n",
            "Continue? [Y/n]y\n",
            "-\n",
            "Input sentence: shout out to my moms for wakin me up so prematurely preciate it\n",
            "Predicted translation: worry\n",
            "Actual translation: <START> worry <END>\n",
            "Continue? [Y/n]y\n",
            "-\n",
            "Input sentence: no need for any explanation really just think about what you can be or do happy encourage 2 m o v e\n",
            "Predicted translation: angry\n",
            "Actual translation: <START> angry <END>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-ff3ee4e54547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Actual translation:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Continue? [Y/n]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mans\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdIBGQXaREnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
